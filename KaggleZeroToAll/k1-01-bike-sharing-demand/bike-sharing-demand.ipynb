{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand\n",
    "https://www.kaggle.com/c/bike-sharing-demand\n",
    "\n",
    "Code based on https://www.kaggle.com/klepacz/titanic/tensor-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem/data description\n",
    "\n",
    "\"You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data\n",
    "\n",
    "First execute data_download.sh to download CSV files:\n",
    "```bash\n",
    "$ sh data_download.sh\n",
    "``` \n",
    "\n",
    "train.csv:\n",
    "```csv\n",
    "datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n",
    "2011-01-01 00:00:00,1,0,0,1,9.84,14.395,81,0,3,13,16\n",
    "2011-01-01 01:00:00,1,0,0,1,9.02,13.635,80,0,8,32,40\n",
    "```\n",
    "\n",
    "test.csv:\n",
    "```csv\n",
    "datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed\n",
    "2011-01-20 00:00:00,1,0,1,1,10.66,11.365,56,26.0027\n",
    "2011-01-20 01:00:00,1,0,1,1,10.66,13.635,56,0\n",
    "```\n",
    "\n",
    "sample_submission.csv:\n",
    "```csv\n",
    "datetime,count\n",
    "2011-01-20 00:00:00,0\n",
    "2011-01-20 01:00:00,0\n",
    "2011-01-20 02:00:00,0\n",
    "2011-01-20 03:00:00,0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code to downlaod and laod \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize x data\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_file(is_test):\n",
    "    if is_test:\n",
    "        data_df = pd.read_csv(\"test.csv\")\n",
    "        data = data_df.values[:, 1:] # Ignore datetime\n",
    "        labels = data_df[\"datetime\"].values\n",
    "    else:\n",
    "        data_df = pd.read_csv(\"train.csv\")\n",
    "        data = data_df.values[:, 1:-3] # Ignore datetime, and count, casual,registered\n",
    "        labels = data_df[\"count\"].values\n",
    "    \n",
    "    print(data_df.head(n=1))\n",
    "    return labels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
      "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
      "\n",
      "   humidity  windspeed  casual  registered  count  \n",
      "0        81        0.0       3          13     16  \n",
      "              datetime  season  holiday  workingday  weather   temp   atemp  \\\n",
      "0  2011-01-20 00:00:00       1        0           1        1  10.66  11.365   \n",
      "\n",
      "   humidity  windspeed  \n",
      "0        56    26.0027  \n",
      "(10886, 8) (6493, 8)\n",
      "(17379, 8)\n",
      "(10886, 8) (6493, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load data and min/max \n",
    "# TODO: clean up this code\n",
    "y_train, x_train = load_file(0)\n",
    "y_train -= 1 # They are 1-7. So let's make it to 0~6\n",
    "y_train = np.expand_dims(y_train, 1)\n",
    "train_len = len(x_train)\n",
    "# Get train file\n",
    "testIds, x_test = load_file(1)\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "x_all = np.vstack((x_train, x_test))\n",
    "print(x_all.shape)\n",
    "\n",
    "x_min_max_all = MinMaxScaler(x_all)\n",
    "x_train = x_min_max_all[:train_len]\n",
    "x_test = x_min_max_all[train_len:]\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model (WIP)\n",
    "Model implementation. It can be divided to several small sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Network Parameters\n",
    "n_input = x_train.shape[1]\n",
    "n_classes = 1  # regression\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])  # 0 ~ 6\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_input, n_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([n_classes]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "Epoch: 01 cost=43078.4788\n",
      "Epoch: 11 cost=27391.4046\n",
      "Epoch: 21 cost=25694.0253\n",
      "Epoch: 31 cost=25098.2386\n",
      "Epoch: 41 cost=24835.8649\n",
      "Epoch: 51 cost=24697.2532\n",
      "Epoch: 61 cost=24612.1160\n",
      "Epoch: 71 cost=24553.2901\n",
      "Epoch: 81 cost=24509.2655\n",
      "Epoch: 91 cost=24474.7160\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 100\n",
    "batch_size = 32\n",
    "display_step = 10\n",
    "step_size = (int)(x_train.shape[0]/batch_size)+1\n",
    "print(step_size)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Loop over step_size\n",
    "        for step in range(step_size):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = x_train[offset:(offset + batch_size), :]\n",
    "            batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_data,\n",
    "                                                          Y: batch_labels})\n",
    "            avg_cost += c / step_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%02d' % (epoch + 1), \"cost={:.4f}\".format(avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    ## 4. Results (creating submission file)\n",
    "    \n",
    "    outputs = sess.run(hypothesis, feed_dict={X: x_test})\n",
    "    submission = ['Id,Cover_Type']\n",
    "\n",
    "    for id, p in zip(testIds, outputs):\n",
    "        submission.append('{0},{1}'.format(id, int(p))) \n",
    "\n",
    "    submission = '\\n'.join(submission)\n",
    "\n",
    "    with open('submission.csv', 'w') as outfile:\n",
    "        outfile.write(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results (creating submission file)\n",
    "(See above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work/exercises\n",
    "* Wide and deep\n",
    "* RNN?\n",
    "* batch norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
