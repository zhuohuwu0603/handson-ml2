{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2, 5: 1})\n",
      "\n",
      "######################\n",
      "#\n",
      "# FINDING KEY CONNECTORS\n",
      "#\n",
      "######################\n",
      "\n",
      "total connections 24\n",
      "number of users 11\n",
      "average connections 2.1818181818181817\n",
      "\n",
      "users sorted by number of friends:\n",
      "[(1, 3), (2, 3), (3, 3), (5, 3), (8, 3), (0, 2), (4, 2), (6, 2), (7, 2), (9, 1), (10, 0)]\n",
      "\n",
      "######################\n",
      "#\n",
      "# DATA SCIENTISTS YOU MAY KNOW\n",
      "#\n",
      "######################\n",
      "\n",
      "friends of friends bad for user 0: [0, 2, 3, 0, 1, 3]\n",
      "friends of friends for user 3: Counter({0: 2, 5: 1})\n",
      "\n",
      "######################\n",
      "#\n",
      "# SALARIES AND TENURES\n",
      "#\n",
      "######################\n",
      "\n",
      "average salary by tenure {0.7: 48000.0, 1.9: 48000.0, 2.5: 60000.0, 4.2: 63000.0, 6.5: 69000.0, 6: 76000.0, 7.5: 76000.0, 8.1: 88000.0, 10: 83000.0, 8.7: 83000.0}\n",
      "average salary by tenure bucket {'between two and five': 61500.0, 'less than two': 48000.0, 'more than five': 79166.66666666667}\n",
      "\n",
      "######################\n",
      "#\n",
      "# MOST COMMON WORDS\n",
      "#\n",
      "######################\n",
      "\n",
      "big 3\n",
      "learning 3\n",
      "java 3\n",
      "data 3\n",
      "python 3\n",
      "statistics 2\n",
      "scikit-learn 2\n",
      "r 2\n",
      "neural 2\n",
      "hbase 2\n",
      "machine 2\n",
      "regression 2\n",
      "probability 2\n",
      "cassandra 2\n",
      "hadoop 2\n",
      "networks 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math, random, csv, json, re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "######\n",
    "#\n",
    "# BOOKS ABOUT DATA\n",
    "#\n",
    "######\n",
    "\n",
    "def is_video(td):\n",
    "    \"\"\"it's a video if it has exactly one pricelabel, and if\n",
    "    the stripped text inside that pricelabel starts with 'Video'\"\"\"\n",
    "    pricelabels = td('span', 'pricelabel')\n",
    "    return (len(pricelabels) == 1 and\n",
    "            pricelabels[0].text.strip().startswith(\"Video\"))\n",
    "\n",
    "def book_info(td):\n",
    "    \"\"\"given a BeautifulSoup <td> Tag representing a book,\n",
    "    extract the book's details and return a dict\"\"\"\n",
    "\n",
    "    title = td.find(\"div\", \"thumbheader\").a.text\n",
    "    by_author = td.find('div', 'AuthorName').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
    "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
    "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
    "\n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"isbn\" : isbn,\n",
    "        \"date\" : date\n",
    "    }\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def scrape(num_pages=31):\n",
    "    base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "           \"data.do?sortby=publicationDate&page=\"\n",
    "\n",
    "    books = []\n",
    "\n",
    "    for page_num in range(1, num_pages + 1):\n",
    "        print(\"souping page\", page_num)\n",
    "        url = base_url + str(page_num)\n",
    "        soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "\n",
    "        for td in soup('td', 'thumbtext'):\n",
    "            if not is_video(td):\n",
    "                books.append(book_info(td))\n",
    "\n",
    "        # now be a good citizen and respect the robots.txt!\n",
    "        sleep(30)\n",
    "\n",
    "    return books\n",
    "\n",
    "def get_year(book):\n",
    "    \"\"\"book[\"date\"] looks like 'November 2014' so we need to\n",
    "    split on the space and then take the second piece\"\"\"\n",
    "    return int(book[\"date\"].split()[1])\n",
    "\n",
    "def plot_years(plt, books):\n",
    "    # 2014 is the last complete year of data (when I ran this)\n",
    "    year_counts = Counter(get_year(book) for book in books\n",
    "                          if get_year(book) <= 2014)\n",
    "\n",
    "    years = sorted(year_counts)\n",
    "    book_counts = [year_counts[year] for year in x]\n",
    "    plt.bar([x - 0.5 for x in years], book_counts)\n",
    "    plt.xlabel(\"year\")\n",
    "    plt.ylabel(\"# of data books\")\n",
    "    plt.title(\"Data is Big!\")\n",
    "    plt.show()\n",
    "\n",
    "##\n",
    "#\n",
    "# APIs\n",
    "#\n",
    "##\n",
    "\n",
    "endpoint = \"https://api.github.com/users/joelgrus/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "####\n",
    "#\n",
    "# Twitter\n",
    "#\n",
    "####\n",
    "\n",
    "from twython import Twython\n",
    "\n",
    "# fill these in if you want to use the code\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "def call_twitter_search_api():\n",
    "\n",
    "    twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "    # search for tweets containing the phrase \"data science\"\n",
    "    for status in twitter.search(q='\"data science\"')[\"statuses\"]:\n",
    "        user = status[\"user\"][\"screen_name\"].encode('utf-8')\n",
    "        text = status[\"text\"].encode('utf-8')\n",
    "        print(user, \":\", text)\n",
    "        print()\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "\n",
    "# appending data to a global variable is pretty poor form\n",
    "# but it makes the example much simpler\n",
    "tweets = []\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    \"\"\"our own subclass of TwythonStreamer that specifies\n",
    "    how to interact with the stream\"\"\"\n",
    "\n",
    "    def on_success(self, data):\n",
    "        \"\"\"what do we do when twitter sends us data?\n",
    "        here data will be a Python object representing a tweet\"\"\"\n",
    "\n",
    "        # only want to collect English-language tweets\n",
    "        if data['lang'] == 'en':\n",
    "            tweets.append(data)\n",
    "\n",
    "        # stop when we've collected enough\n",
    "        if len(tweets) >= 1000:\n",
    "            self.disconnect()\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "\n",
    "def call_twitter_streaming_api():\n",
    "    stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n",
    "                        ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "    # starts consuming public statuses that contain the keyword 'data'\n",
    "    stream.statuses.filter(track='data')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def process(date, symbol, price):\n",
    "        print(date, symbol, price)\n",
    "\n",
    "    print(\"tab delimited stock prices:\")\n",
    "\n",
    "    with open('tab_delimited_stock_prices.txt', 'r', encoding='utf8',newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        # reader = csv.reader(codecs.iterdecode(f, 'utf-8'), delimiter='\\t')\n",
    "        for row in reader:\n",
    "            date = row[0]\n",
    "            symbol = row[1]\n",
    "            closing_price = float(row[2])\n",
    "            process(date, symbol, closing_price)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"colon delimited stock prices:\")\n",
    "\n",
    "    with open('colon_delimited_stock_prices.txt', 'r', encoding='utf8',newline='') as f:\n",
    "        reader = csv.DictReader(f, delimiter=':')\n",
    "        # reader = csv.DictReader(codecs.iterdecode(f, 'utf-8'), delimiter=':')\n",
    "        for row in reader:\n",
    "            date = row[\"date\"]\n",
    "            symbol = row[\"symbol\"]\n",
    "            closing_price = float(row[\"closing_price\"])\n",
    "            process(date, symbol, closing_price)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"writing out comma_delimited_stock_prices.txt\")\n",
    "\n",
    "    today_prices = { 'AAPL' : 90.91, 'MSFT' : 41.68, 'FB' : 64.5 }\n",
    "\n",
    "    with open('comma_delimited_stock_prices.txt','w', encoding='utf8',newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for stock, price in today_prices.items():\n",
    "            writer.writerow([stock, price])\n",
    "\n",
    "    print(\"BeautifulSoup\")\n",
    "    html = requests.get(\"http://www.example.com\").text\n",
    "    soup = BeautifulSoup(html)\n",
    "    print(soup)\n",
    "    print()\n",
    "\n",
    "    print(\"parsing json\")\n",
    "\n",
    "    serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                      \"author\" : \"Joel Grus\",\n",
    "                      \"publicationYear\" : 2014,\n",
    "                      \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "    # parse the JSON to create a Python object\n",
    "    deserialized = json.loads(serialized)\n",
    "    if \"data science\" in deserialized[\"topics\"]:\n",
    "        print(deserialized)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"GitHub API\")\n",
    "    print(\"dates\", dates)\n",
    "    print(\"month_counts\", month_counts)\n",
    "    print(\"weekday_count\", weekday_counts)\n",
    "\n",
    "    last_5_repositories = sorted(repos,\n",
    "                                 key=lambda r: r[\"created_at\"],\n",
    "                                 reverse=True)[:5]\n",
    "\n",
    "    print(\"last five languages\", [repo[\"language\"]\n",
    "                                  for repo in last_5_repositories])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
