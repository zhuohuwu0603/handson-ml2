
import inspect
# from pyspark.sql.functions import * # this is only allowed at module level

def random_forrest():
    print(">>>> This is the begining of def {}().".format(inspect.stack()[0][3]))
    # <b>Dataset location: </b>https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data

    from pyspark.sql import SparkSession

    spark = SparkSession.builder.appName('Predicting whether a person\'s income is greater than $50K').getOrCreate()
    rawData = spark.read.format('csv').option('header', 'false').option('ignoreLeadingWhiteSpace', 'true').load('../datasets/adult.csv')

    dataset = rawData.toDF('Age',
                           'WorkClass',
                           'FnlWgt',
                           'Education',
                           'EducationNum',
                           'MaritalStatus',
                           'Occupation',
                           'Relationship',
                           'Race',
                           'Gender',
                           'CapitalGain',
                           'CapitalLoss',
                           'HoursPerWeek',
                           'NativeCountry',
                           'Label')

    # #### Drop FnlWgt column which does not appear meaningful
    dataset = dataset.drop('FnlWgt')
    dataset = dataset.replace('?', None)
    dataset = dataset.dropna(how='any')

    from pyspark.sql.types import FloatType
    # from pyspark.sql.functions import col

    dataset = dataset.withColumn('Age', dataset['Age'].cast(FloatType()))
    dataset = dataset.withColumn('EducationNum', dataset['EducationNum'].cast(FloatType()))
    dataset = dataset.withColumn('CapitalGain', dataset['CapitalGain'].cast(FloatType()))
    dataset = dataset.withColumn('CapitalLoss', dataset['CapitalLoss'].cast(FloatType()))
    dataset = dataset.withColumn('HoursPerWeek', dataset['HoursPerWeek'].cast(FloatType()))

    # #### Transform categorical fields
    # First use StringIndexer to convert categorical values to indices
    # from pyspark.ml.feature import StringIndexer

    # indexedDF = StringIndexer(
    #     inputCol='WorkClass',
    #     outputCol='WorkClass_index').fit(dataset).transform(dataset)
    #
    # # #### OneHotEncoding
    # # Use the new indexed field to obtain a one-hot-encoded field
    #
    # from pyspark.ml.feature import OneHotEncoder
    # encodedDF = OneHotEncoder(
    #     inputCol="WorkClass_index",
    #     outputCol="WorkClass_encoded").transform(indexedDF)

    # ####  First, split the data into training and test sets
    (trainingData, testData) = dataset.randomSplit([0.8, 0.2])

    categoricalFeatures = [
        'WorkClass',
        'Education',
        'MaritalStatus',
        'Occupation',
        'Relationship',
        'Race',
        'Gender',
        'NativeCountry'
    ]

    # #### Create an array of StringIndexers to convert the categorical values to indices
    from pyspark.ml.feature import StringIndexer
    indexers = [StringIndexer(
        inputCol=column,
        outputCol=column + '_index',
        handleInvalid='keep') for column in categoricalFeatures]

    # #### Create an array of OneHotEncoders to encode the categorical values
    from pyspark.ml.feature import OneHotEncoder
    encoders = [OneHotEncoder(
        inputCol=column + '_index',
        outputCol=column + '_encoded') for column in categoricalFeatures]

    # #### Index the Label field
    labelIndexer = [StringIndexer(inputCol='Label', outputCol='Label_index')]

    # #### Create a pipeline
    # The pipeline contains the array of StringIndexers and OneHotEncoders

    from pyspark.ml import Pipeline
    pipeline = Pipeline(stages=indexers + encoders + labelIndexer)

    # #### View the result of the transformations performed by this pipeline
    # This pipeline can transform our dataset into a format which can be used by our model
    transformedDF = pipeline.fit(trainingData).transform(trainingData)

    transformedDF.printSchema()
    transformedDF.show(5, False)

    # #### Select the required features
    # At this point the dataset contains a lot of additional columns. We select the features needed by our model
    requiredFeatures = [
        'Age',
        'EducationNum',
        'CapitalGain',
        'CapitalLoss',
        'HoursPerWeek',
        'WorkClass_encoded',
        'Education_encoded',
        'MaritalStatus_encoded',
        'Occupation_encoded',
        'Relationship_encoded',
        'Race_encoded',
        'Gender_encoded',
        'NativeCountry_encoded'
    ]

    # #### VectorAssembler
    # VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector
    # * We had previously written our own function to create such a vector

    from pyspark.ml.feature import VectorAssembler
    assembler = VectorAssembler(inputCols=requiredFeatures, outputCol='features')
    transformedDF = assembler.transform(transformedDF)

    # #### Specify our estimator
    from pyspark.ml.classification import RandomForestClassifier
    rf = RandomForestClassifier(labelCol='Label_index', featuresCol='features', maxDepth=5)

    # #### Final Pipeline
    # * The pipeline we built previously only transformed the feature columns
    # * We re-create the pipeline to include the VectorAssembler and the estimator
    #
    # The pipeline to be used to build the model contains all the transformers and ends with the estimator

    pipeline = Pipeline(
        stages=indexers + encoders + labelIndexer + [assembler, rf]
    )

    # #### Train the model
    model = pipeline.fit(trainingData)

    # #### Use the test data for predictions
    predictions = model.transform(testData)
    predictionsDF = predictions.toPandas()
    predictionsDF.head()

    # #### Select the correct label and predictions to evaluate the model
    predictions = predictions.select('Label_index', 'prediction')

    # #### Create an evaluator for our model
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator

    evaluator = MulticlassClassificationEvaluator(
        labelCol='Label_index',
        predictionCol='prediction',
        metricName='accuracy')

    # #### Check the accuracy
    accuracy = evaluator.evaluate(predictions)
    print('Test Accuracy = ', accuracy)

    # #### Examine incorrect predictions
    predictionsDF.loc[
        predictionsDF['Label_index'] != predictionsDF['prediction']
        ]

    print("<<<< This is the end of def {}().".format(inspect.stack()[0][3]))



def linear_regression():
    print(">>>> This is the begining of def {}().".format(inspect.stack()[0][3]))

    # ### Download the Automobile data set
    # <b>Download link:</b> https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data

    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName('Predicting the price of an automobile given a set of features').getOrCreate()
    rawData = spark.read.format('csv').option('header', 'true').load('../datasets/imports-85.data')

    # #### Select the required columns
    # * We can select the specific features we feel are relevant in our dataset
    # * fields such as normalized-losses have been dropped
    # * The numeric fields can be cast as float or any numeric type

    from pyspark.sql.functions import col

    dataset = rawData.select(col('price').cast('float'),
                             col('make'),
                             col('num-of-doors'),
                             col('body-style'),
                             col('drive-wheels'),
                             col('wheel-base').cast('float'),
                             col('curb-weight').cast('float'),
                             col('num-of-cylinders'),
                             col('engine-size').cast('float'),
                             col('horsepower').cast('float'),
                             col('peak-rpm').cast('float')
                             )

    # #### Drop columns with nulls
    # Check number of rows in dataset before and after removal of nulls
    dataset = dataset.replace('?', None).dropna(how='any')
    dataset.count()

    # #### Split dataset into training and test sets
    (trainingData, testData) = dataset.randomSplit([0.8, 0.2])

    # #### List the categorical fields so that we can transform these to encoded values

    categoricalFeatures = ['make',
                           'num-of-doors',
                           'body-style',
                           'drive-wheels',
                           'num-of-cylinders'
                           ]

    # #### Import and implement the required transformers

    from pyspark.ml.feature import StringIndexer
    from pyspark.ml.feature import OneHotEncoder
    from pyspark.ml.feature import VectorAssembler

    # #### Use of handleInvalid in StringIndexer
    # If the model comes across a new label which it hasn't seen in the training phase, it is deemed an "invalid" label. There are different ways of handling this:
    # * handleInvalid='skip' will remove rows with new labels
    # * handleInvalid='error' will cause an error when a new label is encountered
    # * handleInvalid='keep' will create a new index if it encounters a new label (available from Spark 2.2 onwards)

    indexers = [StringIndexer(
        inputCol=column,
        outputCol=column + '_index',
        handleInvalid='keep') for column in categoricalFeatures]

    # #### One-Hot-Encode the features
    encoders = [OneHotEncoder(
        inputCol=column + '_index',
        outputCol=column + '_encoded') for column in categoricalFeatures]

    # #### List all the required features from the transformed dataset

    requiredFeatures = ['make_encoded',
                        'num-of-doors_encoded',
                        'body-style_encoded',
                        'drive-wheels_encoded',
                        'wheel-base',
                        'curb-weight',
                        'num-of-cylinders_encoded',
                        'engine-size',
                        'horsepower',
                        'peak-rpm'
                        ]

    # #### Prepare the feature assembler
    assembler = VectorAssembler(inputCols=requiredFeatures, outputCol='features')

    # #### Linear Regression
    # By setting α properly, elastic net contains both L1 and L2 regularization as special cases.
    # * If the elasticNetParam α is set to 1, it is equivalent to a Lasso model
    # * If α is set to 0, the trained model reduces to a ridge regression model
    #
    # regParam is the regularization variable

    from pyspark.ml.regression import LinearRegression

    lr = LinearRegression(maxIter=100,
                          regParam=1.0,
                          elasticNetParam=0.8,
                          labelCol='price',
                          featuresCol='features')

    # #### Define our pipeline
    # It contains all our transformers plus the model

    from pyspark.ml import Pipeline
    pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])

    model = pipeline.fit(trainingData)

    # ### Extract the model from the pipeline
    # * Our LinearRegression model is not the same as the pipeline model
    # * To extract the LinearRegression model, we get it from the last stage of our pipeline model

    lrModel = model.stages[-1]
    print('Training RMSE = ', lrModel.summary.rootMeanSquaredError)
    print('Training R^2 score = ', lrModel.summary.r2)

    # #### Check the number of features
    # The number will be high as many of our features are one-hot-encoded

    lrModel.numFeatures

    # #### View the coefficients of each feature
    lrModel.coefficients

    # #### There is a coefficient for each feature
    len(lrModel.coefficients)

    # #### Get predictions using our model on the test data

    predictions = model.transform(testData)
    predictionsDF = predictions.toPandas()
    predictionsDF.head()

    # #### The features have been transformed to LibSVM format

    predictionsDF['features'][0]

    # ### Use RegressionEvaluator to evaluate the model
    # * MulticlassClassificationEvaluator is used for classification models
    # * RegressionEvaluator needed to evaluate regression models
    # * <b>metricName </b>can be r2, rmse, mse or mae (mean absolute error)

    from pyspark.ml.evaluation import RegressionEvaluator

    evaluator = RegressionEvaluator(
        labelCol='price',
        predictionCol='prediction',
        metricName='r2')

    r2 = evaluator.evaluate(predictions)
    print('Test R^2 score = ', r2)

    evaluator = RegressionEvaluator(
        labelCol='price',
        predictionCol='prediction',
        metricName='rmse')

    rmse = evaluator.evaluate(predictions)
    print('Test RMSE = ', rmse)

    # #### Compare the actual and predicted values of price

    predictionsPandasDF = predictions.select(
        col('price'),
        col('prediction')
    ).toPandas()

    predictionsPandasDF.head()

    # #### Plot a graph of actual and predicted values of price
    # Note that our predictions dataset is sorted in ascending order of price

    import matplotlib.pyplot as plt

    plt.figure(figsize=(15, 6))
    plt.plot(predictionsPandasDF['price'], label='Actual')
    plt.plot(predictionsPandasDF['prediction'], label='Predicted')
    plt.ylabel('Price')
    plt.legend()
    plt.show()

    # ### Using ParamGrid for hyperparameter tuning
    # The parameters we wish to tweak are:
    # * maxIter
    # * regParam
    # * elasticNetParam - whether a lasso or ridge model will be best
    from pyspark.ml.tuning import ParamGridBuilder

    paramGrid = ParamGridBuilder().addGrid(
        lr.maxIter, [10, 50, 100]).addGrid(
        lr.regParam, [0.1, 0.3, 1.0]).addGrid(
        lr.elasticNetParam, [0.0, 1.0]).build()

    # #### Define the RegressionEvaluator used to evaluate the models
    # We wish to minimize RMSE
    evaluator = RegressionEvaluator(
        labelCol='price',
        predictionCol='prediction',
        metricName='rmse')

    # ### Define the CrossValidator
    # This is used to put all the pieces together
    # * <b>estimator: </b>Can be a standalone estimator or a pipeline with an estimator at the end. We use our pipeline
    # * <b>estimatorParamMaps: </b>We add our paramGrid in order to build models with different combinations of the parameters
    # * <b>evaluator: </b>To evaluate each model, we specify our evaluator

    from pyspark.ml.tuning import CrossValidator

    crossval = CrossValidator(estimator=pipeline,
                              estimatorParamMaps=paramGrid,
                              evaluator=evaluator,
                              numFolds=3)

    # #### Train each of our models with the training data
    # After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset
    model = crossval.fit(trainingData)

    # #### To examine our best model, we extract it from the pipeline
    lrModel = model.bestModel.stages[-1]

    # #### Get the values of the "best" parameters
    # Unfortunately, extracting these values is a bit awkward as we need to access the \_java\_obj object

    print('maxIter=', lrModel._java_obj.getMaxIter())
    print('elasticNetParam=', lrModel._java_obj.getElasticNetParam())
    print('regParam=', lrModel._java_obj.getRegParam())

    # #### Make predictions using our "best" model

    predictions = model.transform(testData)
    predictionsDF = predictions.toPandas()
    predictionsDF.head()

    # #### Evaluate the model on it's R-square score and RMSE
    evaluator = RegressionEvaluator(
        labelCol='price',
        predictionCol='prediction',
        metricName='r2')

    rsquare = evaluator.evaluate(predictions)
    print("Test R^2 score = %g" % rsquare)

    evaluator = RegressionEvaluator(
        labelCol='price',
        predictionCol='prediction',
        metricName='rmse')

    rmse = evaluator.evaluate(predictions)
    print('Test RMSE = ', rmse)

    # #### Compare actual and predicted values of price
    predictionsPandasDF = predictions.select(
        col('price'),
        col('prediction')).toPandas()

    predictionsPandasDF.head()

    # #### Perform the comparison using a graph
    plt.figure(figsize=(15, 6))
    plt.plot(predictionsPandasDF['price'], label='Actual')
    plt.plot(predictionsPandasDF['prediction'], label='Predicted')
    plt.ylabel('Price')
    plt.legend()
    plt.show()
    print("<<<< This is the end of def {}().".format(inspect.stack()[0][3]))

def kmeans_clustering():
    print(">>>> This is the begining of def {}().".format(inspect.stack()[0][3]))
    # ### Download the dataset
    # <b>Dataset location: </b>https://www.kaggle.com/c/3136/download/train.csv

    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName('Examine data about passengers on the Titanic').getOrCreate()
    rawData = spark.read.format('csv').option('header', 'true').load('../datasets/titanic.csv')

    rawData.toPandas().head()

    # #### Select the columns which we required
    # Also cast the numeric values as float

    from pyspark.sql.functions import col
    dataset = rawData.select(col('Survived').cast('float'),
                             col('Pclass').cast('float'),
                             col('Sex'),
                             col('Age').cast('float'),
                             col('Fare').cast('float'),
                             col('Embarked')
                             )
    dataset.toPandas().head()

    # #### Drop rows containing missing values
    dataset = dataset.replace('?', None).dropna(how='any')

    # #### Define StringIndexers for categorical columns
    from pyspark.ml.feature import StringIndexer

    dataset = StringIndexer(
        inputCol='Sex',
        outputCol='Gender',
        handleInvalid='keep').fit(dataset).transform(dataset)

    dataset = StringIndexer(
        inputCol='Embarked',
        outputCol='Boarded',
        handleInvalid='keep').fit(dataset).transform(dataset)

    dataset.toPandas().head()

    # #### Drop the redundant columns
    dataset = dataset.drop('Sex')
    dataset = dataset.drop('Embarked')

    dataset.toPandas().head()

    # #### Define the required features to use in the VectorAssembler
    # Since we are only examining data and not making predictions, we include all columns

    requiredFeatures = ['Survived',
                        'Pclass',
                        'Age',
                        'Fare',
                        'Gender',
                        'Boarded'
                        ]

    # #### The VectorAssembler vectorises all the features
    # The transformed data will be used for clustering
    from pyspark.ml.feature import VectorAssembler
    assembler = VectorAssembler(inputCols=requiredFeatures, outputCol='features')

    # #### Transorm our dataset for use in our clustering algorithm
    transformed_data = assembler.transform(dataset)
    transformed_data.toPandas().head()

    # ### Define the clustering model
    # Use K-means clustering
    # * <b>k: </b>Defines the number of clusters
    # * <b>seed: </b>This value is used to set the cluster centers. A different value of seed for the same k will result in clusters being defined differently. In order to reproduce similar clusters when re-running the clustering algorithm use the same values of k and seed
    from pyspark.ml.clustering import KMeans

    kmeans = KMeans(k=5, seed=3)
    model = kmeans.fit(transformed_data)

    # #### Create the clusters using the model
    clusterdData = model.transform(transformed_data)

    # #### Use ClusteringEvaluator to evaluate the clusters
    # <b>From Wikipedia: </b>The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.
    from pyspark.ml.evaluation import ClusteringEvaluator

    evaluator = ClusteringEvaluator()
    silhouette = evaluator.evaluate(clusterdData)
    print('Silhouette with squared euclidean distance = ', silhouette)

    #### View the cluster centers for each of the features
    centers = model.clusterCenters()
    print('Cluster Centers: ')
    for center in centers:
        print(center)

    # #### View the output of the KMeans model
    # The prediction field denotes the cluster number
    clusterdData.toPandas().head()

    # #### Get the average of each feature in the original data
    # This is the equivalent of the cluster center when our dataset is one big cluster
    # * We import all sql functions as we need the avg and count functions among others
    from pyspark.sql.functions import sum, count, avg, expr
    dataset.select(avg('Survived'),
                   avg('Pclass'),
                   avg('Age'),
                   avg('Fare'),
                   avg('Gender'),
                   avg('Boarded')).toPandas()

    # #### A more intuitive way to view the cluster centers in our clusterdData
    # * We group by clusterID (prediction) and compute the average of all features
    # * We do a count of values in each cluster

    clusterdData.groupBy('prediction').agg(avg('Survived'),
                                           avg('Pclass'),
                                           avg('Age'),
                                           avg('Fare'),
                                           avg('Gender'),
                                           avg('Boarded'),
                                           count('prediction')
                                           ).orderBy('prediction').toPandas()

    # #### Examine all rows in one of the clusters
    clusterdData.filter(clusterdData.prediction == 1).toPandas()
    print("<<<< This is the end of def {}().".format(inspect.stack()[0][3]))

def test():
    print(">>>> This is the begining of def {}().".format(inspect.stack()[0][3]))

    print("<<<< This is the end of def {}().".format(inspect.stack()[0][3]))


# def foo():
#    print(inspect.stack()[0][3])
#    print(inspect.stack()[1][3]) #will give the caller of foos name, if something called foo

if __name__ == '__main__':
    # random_forrest()
    # linear_regression()
    kmeans_clustering()
    # foo()
    test()