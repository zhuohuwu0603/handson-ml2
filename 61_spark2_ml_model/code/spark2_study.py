
def random_forrest():
    print(">>>> This is the begining of random_forrest.")
    # <b>Dataset location: </b>https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data

    from pyspark.sql import SparkSession

    spark = SparkSession.builder.appName('Predicting whether a person\'s income is greater than $50K').getOrCreate()
    rawData = spark.read.format('csv').option('header', 'false').option('ignoreLeadingWhiteSpace', 'true').load('../datasets/adult.csv')

    dataset = rawData.toDF('Age',
                           'WorkClass',
                           'FnlWgt',
                           'Education',
                           'EducationNum',
                           'MaritalStatus',
                           'Occupation',
                           'Relationship',
                           'Race',
                           'Gender',
                           'CapitalGain',
                           'CapitalLoss',
                           'HoursPerWeek',
                           'NativeCountry',
                           'Label')

    # #### Drop FnlWgt column which does not appear meaningful
    dataset = dataset.drop('FnlWgt')
    dataset = dataset.replace('?', None)
    dataset = dataset.dropna(how='any')

    from pyspark.sql.types import FloatType
    # from pyspark.sql.functions import col

    dataset = dataset.withColumn('Age', dataset['Age'].cast(FloatType()))
    dataset = dataset.withColumn('EducationNum', dataset['EducationNum'].cast(FloatType()))
    dataset = dataset.withColumn('CapitalGain', dataset['CapitalGain'].cast(FloatType()))
    dataset = dataset.withColumn('CapitalLoss', dataset['CapitalLoss'].cast(FloatType()))
    dataset = dataset.withColumn('HoursPerWeek', dataset['HoursPerWeek'].cast(FloatType()))

    # #### Transform categorical fields
    # First use StringIndexer to convert categorical values to indices
    # from pyspark.ml.feature import StringIndexer

    # indexedDF = StringIndexer(
    #     inputCol='WorkClass',
    #     outputCol='WorkClass_index').fit(dataset).transform(dataset)
    #
    # # #### OneHotEncoding
    # # Use the new indexed field to obtain a one-hot-encoded field
    #
    # from pyspark.ml.feature import OneHotEncoder
    # encodedDF = OneHotEncoder(
    #     inputCol="WorkClass_index",
    #     outputCol="WorkClass_encoded").transform(indexedDF)

    # ####  First, split the data into training and test sets
    (trainingData, testData) = dataset.randomSplit([0.8, 0.2])

    categoricalFeatures = [
        'WorkClass',
        'Education',
        'MaritalStatus',
        'Occupation',
        'Relationship',
        'Race',
        'Gender',
        'NativeCountry'
    ]

    # #### Create an array of StringIndexers to convert the categorical values to indices
    from pyspark.ml.feature import StringIndexer
    indexers = [StringIndexer(
        inputCol=column,
        outputCol=column + '_index',
        handleInvalid='keep') for column in categoricalFeatures]

    # #### Create an array of OneHotEncoders to encode the categorical values
    from pyspark.ml.feature import OneHotEncoder
    encoders = [OneHotEncoder(
        inputCol=column + '_index',
        outputCol=column + '_encoded') for column in categoricalFeatures]

    # #### Index the Label field
    labelIndexer = [StringIndexer(inputCol='Label', outputCol='Label_index')]

    # #### Create a pipeline
    # The pipeline contains the array of StringIndexers and OneHotEncoders

    from pyspark.ml import Pipeline
    pipeline = Pipeline(stages=indexers + encoders + labelIndexer)

    # #### View the result of the transformations performed by this pipeline
    # This pipeline can transform our dataset into a format which can be used by our model
    transformedDF = pipeline.fit(trainingData).transform(trainingData)

    transformedDF.printSchema()
    transformedDF.show(5, False)

    # #### Select the required features
    # At this point the dataset contains a lot of additional columns. We select the features needed by our model
    requiredFeatures = [
        'Age',
        'EducationNum',
        'CapitalGain',
        'CapitalLoss',
        'HoursPerWeek',
        'WorkClass_encoded',
        'Education_encoded',
        'MaritalStatus_encoded',
        'Occupation_encoded',
        'Relationship_encoded',
        'Race_encoded',
        'Gender_encoded',
        'NativeCountry_encoded'
    ]

    # #### VectorAssembler
    # VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector
    # * We had previously written our own function to create such a vector

    from pyspark.ml.feature import VectorAssembler
    assembler = VectorAssembler(inputCols=requiredFeatures, outputCol='features')
    transformedDF = assembler.transform(transformedDF)

    # #### Specify our estimator
    from pyspark.ml.classification import RandomForestClassifier
    rf = RandomForestClassifier(labelCol='Label_index', featuresCol='features', maxDepth=5)

    # #### Final Pipeline
    # * The pipeline we built previously only transformed the feature columns
    # * We re-create the pipeline to include the VectorAssembler and the estimator
    #
    # The pipeline to be used to build the model contains all the transformers and ends with the estimator

    pipeline = Pipeline(
        stages=indexers + encoders + labelIndexer + [assembler, rf]
    )

    # #### Train the model
    model = pipeline.fit(trainingData)

    # #### Use the test data for predictions
    predictions = model.transform(testData)
    predictionsDF = predictions.toPandas()
    predictionsDF.head()

    # #### Select the correct label and predictions to evaluate the model
    predictions = predictions.select('Label_index', 'prediction')

    # #### Create an evaluator for our model
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator

    evaluator = MulticlassClassificationEvaluator(
        labelCol='Label_index',
        predictionCol='prediction',
        metricName='accuracy')

    # #### Check the accuracy
    accuracy = evaluator.evaluate(predictions)
    print('Test Accuracy = ', accuracy)

    # #### Examine incorrect predictions
    predictionsDF.loc[
        predictionsDF['Label_index'] != predictionsDF['prediction']
        ]

    print("<<<< This is the end of random_forrest.")



def test():
    print(">>>> This is the begining of random_forrest.")

    print("<<<< This is the end of random_forrest.")




if __name__ == '__main__':
    random_forrest()