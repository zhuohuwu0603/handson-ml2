{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram sentences\n",
      "0 But that's going to be current to generate a large searches correlates what that can be saying you can you want to figure out what's important role in enabling agile practices are then combine entrepreneurship with hundreds of track titles artists album .\n",
      "1 CDDB views music by Jeff Hammerbacher said on the Philadelphia County by analyzing musical problem isn't the relational database .\n",
      "2 If anything from machine consumable .\n",
      "3 It was probably generated by their path .\n",
      "4 Roger Magoulas who runs the metadata track titles .\n",
      "5 The result .\n",
      "6 Amazon understands that nobody remembers says that nobody remembers says that gave them open source R is really necessary for working with gathering data collection tools like the ability to be nice if it necessary to do data scientists particularly physicists rather than for distributing an audio stream processing companies banks and other disciplines it arrives and are easier to use .\n",
      "7 Point of them open source the low 1 The Turk is a distributed across many of data is a multistage processing companies like Infochimps and added value in size of data might like a schema in which lets developers have 1 The first gigabyte disk drives in .\n",
      "8 It then use one stop information platform though these applications to many modern web applications it's figuring out incrementally rather than sales to develop and the data sources and 5 .\n",
      "9 Five years ago .\n",
      "\n",
      "trigram sentences\n",
      "0 At O'Reilly we frequently combine publishing industry .\n",
      "1 Her job as scientist at bit .\n",
      "2 Edward Tufte's Visual Display of Quantitative Information is the heart of what might be interesting .\n",
      "3 Information platforms are similar to traditional data warehouses but different .\n",
      "4 Her job as scientist at LinkedIn .\n",
      "5 Even a relatively small simple program that looked at members' profiles and made recommendations accordingly .\n",
      "6 It has excellent graphics facilities CRAN includes parsers for many kinds of data science .\n",
      "7 Data conditioning can involve cleaning up messy HTML with tools like Beautiful Soup natural language processing fails you can split your task up into smaller problems .\n",
      "8 If you have a strong mathematical background computing skills and come from a number of cores .\n",
      "9 Whether we're talking about web server logs tweet streams online transaction records citizen science data from Nielsen BookScan with our own sales data publicly available Amazon data and even job data to place the foreclosures on a map another data source and group them by neighborhood valuation neighborhood per capita income and other data sources to verify them .\n",
      "\n",
      "grammar sentences\n",
      "0 linear regression about linear regression is\n",
      "1 big regression about big data science learns\n",
      "2 Python trains regression\n",
      "3 logistic logistic regression about logistic regression about logistic Python tests\n",
      "4 linear data science about logistic data science tests big linear linear linear data science about big Python near big data science near linear data science near logistic regression\n",
      "5 Python learns linear big Python about big data science about logistic Python\n",
      "6 big linear data science near logistic regression near logistic data science trains linear regression about big Python\n",
      "7 big linear data science about linear Python about logistic regression trains\n",
      "8 logistic big Python near big data science near big data science trains Python\n",
      "9 big linear big big Python about linear Python about big data science near big regression about big data science learns\n",
      "\n",
      "gibbs sampling\n",
      "(4, 10) 21 30\n",
      "(6, 9) 29 23\n",
      "(2, 6) 32 34\n",
      "(4, 9) 30 16\n",
      "(4, 8) 23 26\n",
      "(4, 5) 31 25\n",
      "(2, 8) 22 24\n",
      "(6, 11) 20 20\n",
      "(2, 7) 32 43\n",
      "(1, 4) 23 31\n",
      "(6, 7) 35 36\n",
      "(5, 9) 27 38\n",
      "(5, 6) 29 23\n",
      "(3, 9) 20 26\n",
      "(1, 6) 35 32\n",
      "(6, 12) 28 32\n",
      "(3, 7) 32 22\n",
      "(2, 5) 20 33\n",
      "(5, 8) 27 29\n",
      "(6, 8) 32 35\n",
      "(1, 2) 36 19\n",
      "(5, 10) 31 28\n",
      "(4, 6) 24 26\n",
      "(4, 7) 27 21\n",
      "(6, 10) 23 23\n",
      "(5, 7) 26 30\n",
      "(3, 8) 25 27\n",
      "(1, 5) 32 38\n",
      "(3, 6) 39 23\n",
      "(1, 7) 31 33\n",
      "(1, 3) 23 20\n",
      "(2, 3) 23 25\n",
      "(5, 11) 32 26\n",
      "(3, 5) 23 23\n",
      "(3, 4) 32 31\n",
      "(2, 4) 25 29\n",
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 Spark 1\n",
      "0 MapReduce 1\n",
      "0 HBase 1\n",
      "0 deep learning 1\n",
      "0 Storm 1\n",
      "0 C++ 1\n",
      "0 programming languages 1\n",
      "0 Cassandra 1\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 machine learning 2\n",
      "1 HBase 2\n",
      "1 MongoDB 2\n",
      "1 decision trees 1\n",
      "1 Cassandra 1\n",
      "1 MySQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "1 deep learning 1\n",
      "1 NoSQL 1\n",
      "1 databases 1\n",
      "1 numpy 1\n",
      "2 regression 3\n",
      "2 scikit-learn 2\n",
      "2 R 2\n",
      "2 Python 2\n",
      "2 libsvm 2\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 statsmodels 2\n",
      "3 pandas 2\n",
      "3 R 2\n",
      "3 Python 2\n",
      "3 artificial intelligence 1\n",
      "3 C++ 1\n",
      "3 theory 1\n",
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 7\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "databases 5\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "databases 2\n",
      "machine learning 2\n",
      "statistics 2\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "statistics 3\n",
      "machine learning 2\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "Big Data and programming languages 3\n",
      "machine learning 3\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "statistics 3\n",
      "machine learning 1\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "databases 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "statistics 3\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "statistics 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "statistics 3\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "databases 5\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "machine learning 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math, random, re\n",
    "from collections import defaultdict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def plot_resumes(plt):\n",
    "    data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
    "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
    "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
    "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
    "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
    "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
    "         (\"thought leadership\", 35, 35)]\n",
    "\n",
    "    def text_size(total):\n",
    "        \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\"\n",
    "        return 8 + total / 200 * 20\n",
    "\n",
    "    for word, job_popularity, resume_popularity in data:\n",
    "        plt.text(job_popularity, resume_popularity, word,\n",
    "                 ha='center', va='center',\n",
    "                 size=text_size(job_popularity + resume_popularity))\n",
    "    plt.xlabel(\"Popularity on Job Postings\")\n",
    "    plt.ylabel(\"Popularity on Resumes\")\n",
    "    plt.axis([0, 100, 0, 100])\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# n-gram models\n",
    "#\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\", \"'\")\n",
    "\n",
    "def get_document():\n",
    "\n",
    "    url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "    content = soup.find(\"div\", \"article-body\")         # find article-body div\n",
    "    regex = r\"[\\w']+|[\\.]\"                             # matches a word or a period\n",
    "\n",
    "    document = []\n",
    "\n",
    "\n",
    "    for paragraph in content(\"p\"):\n",
    "        words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "        document.extend(words)\n",
    "\n",
    "    return document\n",
    "\n",
    "def generate_using_bigrams(transitions):\n",
    "    current = \".\"   # this means the next word will start a sentence\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]    # bigrams (current, _)\n",
    "        current = random.choice(next_word_candidates)  # choose one at random\n",
    "        result.append(current)                         # append it to results\n",
    "        if current == \".\": return \" \".join(result)     # if \".\" we're done\n",
    "\n",
    "def generate_using_trigrams(starts, trigram_transitions):\n",
    "    current = random.choice(starts)   # choose a random starting word\n",
    "    prev = \".\"                        # and precede it with a '.'\n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)\n",
    "\n",
    "def is_terminal(token):\n",
    "    return token[0] != \"_\"\n",
    "\n",
    "def expand(grammar, tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        # ignore terminals\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # choose a replacement at random\n",
    "        replacement = random.choice(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "        return expand(grammar, tokens)\n",
    "\n",
    "    # if we get here we had all terminals and are done\n",
    "    return tokens\n",
    "\n",
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])\n",
    "\n",
    "#\n",
    "# Gibbs Sampling\n",
    "#\n",
    "\n",
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        # if the total is 7 or less, the first die is equally likely to be\n",
    "        # 1, 2, ..., (total - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # if the total is 7 or more, the first die is equally likely to be\n",
    "        # (total - 6), (total - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iters=100):\n",
    "    x, y = 1, 2 # doesn't really matter\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts\n",
    "\n",
    "#\n",
    "# TOPIC MODELING\n",
    "#\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()       # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                        # return the smallest i such that\n",
    "        if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd\n",
    "\n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "K = 4\n",
    "\n",
    "document_topic_counts = [Counter()\n",
    "                         for _ in documents]\n",
    "\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"the fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the k-th topic\"\"\"\n",
    "\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    document = get_document()\n",
    "\n",
    "    bigrams = list(zip(document, document[1:]))\n",
    "    transitions = defaultdict(list)\n",
    "    for prev, current in bigrams:\n",
    "        transitions[prev].append(current)\n",
    "\n",
    "    random.seed(0)\n",
    "    print(\"bigram sentences\")\n",
    "    for i in range(10):\n",
    "        print(i, generate_using_bigrams(transitions))\n",
    "    print()\n",
    "\n",
    "    # trigrams\n",
    "\n",
    "    trigrams = list(zip(document, document[1:], document[2:]))\n",
    "    trigram_transitions = defaultdict(list)\n",
    "    starts = []\n",
    "\n",
    "    for prev, current, next in trigrams:\n",
    "\n",
    "        if prev == \".\":              # if the previous \"word\" was a period\n",
    "            starts.append(current)   # then this is a start word\n",
    "\n",
    "        trigram_transitions[(prev, current)].append(next)\n",
    "\n",
    "    print(\"trigram sentences\")\n",
    "    for i in range(10):\n",
    "        print(i, generate_using_trigrams(starts, trigram_transitions))\n",
    "    print()\n",
    "\n",
    "    grammar = {\n",
    "        \"_S\"  : [\"_NP _VP\"],\n",
    "        \"_NP\" : [\"_N\",\n",
    "                 \"_A _NP _P _A _N\"],\n",
    "        \"_VP\" : [\"_V\",\n",
    "                 \"_V _NP\"],\n",
    "        \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
    "        \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
    "        \"_P\"  : [\"about\", \"near\"],\n",
    "        \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "    }\n",
    "\n",
    "    print(\"grammar sentences\")\n",
    "    for i in range(10):\n",
    "        print(i, \" \".join(generate_sentence(grammar)))\n",
    "    print()\n",
    "\n",
    "    print(\"gibbs sampling\")\n",
    "    comparison = compare_distributions()\n",
    "    for roll, (gibbs, direct) in comparison.items():\n",
    "        print(roll, gibbs, direct)\n",
    "\n",
    "\n",
    "    # topic MODELING\n",
    "\n",
    "    for k, word_counts in enumerate(topic_word_counts):\n",
    "        for word, count in word_counts.most_common():\n",
    "            if count > 0: print(k, word, count)\n",
    "\n",
    "    topic_names = [\"Big Data and programming languages\",\n",
    "                   \"databases\",\n",
    "                   \"machine learning\",\n",
    "                   \"statistics\"]\n",
    "\n",
    "    for document, topic_counts in zip(documents, document_topic_counts):\n",
    "        print(document)\n",
    "        for topic, count in topic_counts.most_common():\n",
    "            if count > 0:\n",
    "                print(topic_names[topic], count)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
